
1 file = 8 blocked Twitter dump files ~= 800 MB
VM image: all-in-one, 1 core, agent delay = controller delay = 1s
number of VM = num files

synthetic dataset (one flumedata block was duplicated 80 times)

+-----------+-----------------------+-----------------------+--------------------+
| num files |  seq sorting time, s  |  par sorting time, s  |    mid speedup     |
+-----------+-----------------------+-----------------------+--------------------+
     2         79.9039
	           73.0112    73.6
			   67.8866
				   
	 3         108.996
               108.917    109.1
               109.506			   
	 
				   
	 4         151.494
	           154.19     153.0
			   153.332
	 
	 5         215.775
	           206.665    206.2
			   196.046
	 
	 6         244.9
	           261.267    251.2
			   247.379
	 
	 7         305.599
	           296.523    300.8
			   300.189
	           
	 8         362.122
	           358.242    359.0
			   356.637
	 
	 9         401.877
	           418.618    412.1
			   415.909
	  
	 10        472.994
	           457.661    466.7
			   469.448
			   
    real dataset (80 flumedata blocks, 8 blocks in one file)

+-----------+-----------------------+-----------------------+--------------------+
| num files |  seq sorting time, s  |  par sorting time, s  |    mid speedup     |
+-----------+-----------------------+-----------------------+--------------------+
     2        68.4562 
	           
			   
				   
	 3        114.838 
               
               		   
	 
				   
	 4        183.547       
	           
			   
	 
	 5        245.916 
	           
			  
	 
	 6        330.048       
	           
			   
	 
	 7        409.327 
	           
			   
	           
	 8        484.539 
	           
			   
	 
	 9        599.583 
	           
			   
	         
	 10       814.518
              	 
	           
			   	   